from sentence_transformers import SentenceTransformer
import faiss
import numpy as np


import zipfile
from bs4 import BeautifulSoup
import re
# Current path jan.zip
zip_path = r""

# loading embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Storing docs
docs = []

with zipfile.ZipFile(zip_path, "r") as z:
    html_files = [f for f in z.namelist() if f.endswith(".html")]

    file_word_counts = {}  # dictionary: {filename: Counter(words)}

    for html_file in html_files:
        with z.open(html_file) as f:
            content = f.read().decode("utf-8", errors="ignore")
            soup = BeautifulSoup(content, "html.parser")

            # Extract plain text
            text = soup.get_text(separator=" ", strip=True)

            # Keep only words (letters only)
            words = re.findall(r"\b[a-zA-Z]+\b", text.lower())

            docs.append(words)

# print(docs)


# Embed documents
doc_embeddings = model.encode(docs, convert_to_numpy=True)

# Building index using FAISS
d = doc_embeddings.shape[1]  # embedding dimension
index = faiss.IndexFlatL2(d)  # L2 distance
index.add(doc_embeddings)

# Query
query = input("What to search: ")
query_vector = model.encode([query], convert_to_numpy=True)

# Search (top 2 results)
k = 2
distances, indices = index.search(query_vector, k)

# Showing results
for idx in indices[0]:
    print("-----------------------------")
    print(docs[idx])
